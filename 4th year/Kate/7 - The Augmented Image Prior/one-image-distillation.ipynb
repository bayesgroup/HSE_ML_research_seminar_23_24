{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install timm detectors wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime as dt\n\nimport detectors\nimport timm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.datasets import CIFAR100, ImageFolder\nimport torchvision.transforms as transforms\n\nfrom tqdm import tqdm\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2023-10-02T08:47:13.785827Z","iopub.execute_input":"2023-10-02T08:47:13.786495Z","iopub.status.idle":"2023-10-02T08:47:33.300465Z","shell.execute_reply.started":"2023-10-02T08:47:13.786455Z","shell.execute_reply":"2023-10-02T08:47:33.299449Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"def save_model(path, num_epochs, model, optimizer, scheduler=None):\n    '''Save on GPU'''\n    data = {\n        'num_epochs': num_epochs,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None\n    }\n    torch.save(data, path)\n\n\ndef load_model(path, device, model, optimizer=None, scheduler=None):\n    '''Load on GPU'''\n    data = torch.load(path)\n    model.load_state_dict(data['model_state_dict'])\n    model.to(device)\n    if optimizer is not None:\n        optimizer.load_state_dict(data['optimizer_state_dict'])\n    if scheduler is not None:\n        scheduler.load_state_dict(data['scheduler_state_dict'])\n    return data['num_epochs']\n\n\n@torch.no_grad()\ndef validation(model, test_loader, device):\n    model.eval()\n\n    val_loss, val_acc, test_set_size = 0.0, 0.0, 0\n    for batch, labels in tqdm(test_loader):\n        batch = batch.to(device)\n        labels = labels.to(device)\n        preds = model(batch)\n        \n        val_loss += F.cross_entropy(preds, labels) * len(batch)\n        val_acc += (preds.argmax(dim=1) == labels).sum()\n        test_set_size += len(batch)\n\n    return {\n        'student_test_loss': val_loss / test_set_size, \n        'student_test_acc': val_acc / test_set_size,\n        'teacher_test_acc': 0.7926\n    }\n\n\ndef distill(teacher, student, train_loader, test_loader, kd_loss, optimizer, scheduler, \n            n_epochs, valid_period, save_period, temp, device, wandb_init_data):\n    with wandb.init(**wandb_init_data) as run:    \n        print(f'Training started: {dt.datetime.now()}')\n        for epoch in range(n_epochs):\n            student.train()\n            for batch, labels in train_loader:\n                batch = batch.to(device)\n                \n                optimizer.zero_grad()\n\n                with torch.inference_mode():\n                    teacher_predictions = teacher(batch)\n                student_predictions = student(batch)\n                loss = kd_loss(F.log_softmax(student_predictions / temp, dim=1),\n                            F.softmax(teacher_predictions / temp, dim=1))\n                loss.backward()\n                optimizer.step()\n\n            if (epoch + 1) % valid_period == 0:\n                print(f'{epoch + 1} training epochs finished\\nValidation started: {dt.datetime.now()}')\n                with torch.inference_mode():\n                    wandb_metrics_value = validation(student, test_loader, device)\n                    wandb.log(wandb_metrics_value)\n                print(f'\\nValidation finished: {dt.datetime.now()}')\n            \n            if (epoch + 1) % save_period == 0:\n                ckpt_filename = f'{epoch + 1}epochs.pt'\n                save_model(ckpt_filename, epoch + 1, student, optimizer, scheduler)\n                wandb.save(ckpt_filename)\n                print(f'\\nCheckpoint saved after {epoch + 1} epochs\\n')","metadata":{"execution":{"iopub.status.busy":"2023-10-02T08:47:33.301827Z","iopub.execute_input":"2023-10-02T08:47:33.302817Z","iopub.status.idle":"2023-10-02T08:47:33.317142Z","shell.execute_reply.started":"2023-10-02T08:47:33.302778Z","shell.execute_reply":"2023-10-02T08:47:33.316150Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nteacher = timm.create_model(\"resnet18_cifar100\", pretrained=True).to(device)\nteacher.eval()\nstudent = timm.create_model(\"resnet18_cifar100\", pretrained=False).to(device)\n\ntrain_transform = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n                         std=[0.2023, 0.1994, 0.2010])\n])\ntest_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408],\n                         std=[0.2675, 0.2565, 0.2761])\n])\n\nbatch_size = 512\nnum_workers = 2\n\npath_to_sigle_image_dataset = '/kaggle/input/ameyoko'\nsingle_image_dataset = ImageFolder(path_to_sigle_image_dataset, transform=train_transform)\ncifar_train_dataset = CIFAR100('.', train=True, transform=train_transform, download=True)\ntest_dataset = CIFAR100('.', train=False, transform=test_transforms, download=True)\n\nsingle_image_loader = DataLoader(single_image_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\ncifar_train_loader = DataLoader(cifar_train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\noptimizer = torch.optim.Adam(student.parameters(), lr=0.001)\nkd_loss = nn.KLDivLoss(reduction=\"batchmean\")\nscheduler = None\n\ntemp = 8.0","metadata":{"execution":{"iopub.status.busy":"2023-10-02T08:47:33.320077Z","iopub.execute_input":"2023-10-02T08:47:33.320685Z","iopub.status.idle":"2023-10-02T08:48:12.831373Z","shell.execute_reply.started":"2023-10-02T08:47:33.320649Z","shell.execute_reply":"2023-10-02T08:48:12.830350Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Downloading: \"https://huggingface.co/edadaltocg/resnet18_cifar100/resolve/main/pytorch_model.bin\" to /root/.cache/torch/hub/checkpoints/resnet18_cifar100.pth\n100%|██████████| 42.9M/42.9M [00:00<00:00, 76.0MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./cifar-100-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 169001437/169001437 [00:02<00:00, 78087317.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./cifar-100-python.tar.gz to .\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ameyoko distillation","metadata":{}},{"cell_type":"code","source":"n_epochs = 300\nvalid_period = 1\nsave_period = 20\n\nwandb_init_data = {\n    'project': 'one_image_distillation',\n    'name': 'distillation on ameyoko',\n    'save_code': True,\n    'config': {\n        'model': 'ResNet18',\n        'optimizer': optimizer,\n        'scheduler': scheduler,\n        'valid_period': valid_period,\n        'dataset': 'ameyoko',\n        'num_epochs': n_epochs,\n        'dataloader_num_workers': num_workers,\n    }\n}\n\ndistill(teacher, student, single_image_loader, test_loader, kd_loss, optimizer, scheduler, \n        n_epochs, valid_period, save_period, temp, device, wandb_init_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# cifar-train-split distillation","metadata":{}},{"cell_type":"code","source":"n_epochs = 300\nvalid_period = 1\nsave_period = 20\n\nwandb_init_data = {\n    'project': 'one_image_distillation',\n    'name': 'distillation on cifar-train-split',\n    'save_code': True,\n    'config': {\n        'model': 'ResNet18',\n        'optimizer': optimizer,\n        'scheduler': scheduler,\n        'valid_period': valid_period,\n        'dataset': 'cifar-train-split',\n        'num_epochs': n_epochs,\n        'dataloader_num_workers': num_workers,\n    }\n}\n\ndistill(teacher, student, cifar_train_loader, test_loader, kd_loss, optimizer, scheduler, \n        n_epochs, valid_period, save_period, temp, device, wandb_init_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# resuming run","metadata":{}},{"cell_type":"code","source":"project_name = 'one_image_distillation'\nrun_id = 'i0yw0j67'\nn_epochs = 1000\nvalid_period = 1\nsave_period = 20\n\n# load checkpoint from wandb\nlast_ckpt = wandb.restore('300epochs.pt', run_path=f\"nik-fedorov/{project_name}/{run_id}\")\nload_model(last_ckpt.name, device, student, optimizer)\n\n# set wandb_init_data for resuming\nwandb_init_data = {\n    'project': project_name,\n    'id': run_id,\n    'resume': 'must',\n    'save_code': True\n}\n\n# resume training\ndistill(teacher, student, single_image_loader, test_loader, kd_loss, optimizer, scheduler, \n        n_epochs, valid_period, save_period, temp, device, wandb_init_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}